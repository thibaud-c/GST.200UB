{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03f0eae2",
   "metadata": {},
   "source": [
    "# Exercise: Reproducing 'How close is \"close\"?' (Shingleton & Basiri, 2025)\n",
    "\n",
    "This notebook guides you through a **small-scale reproduction** of the core analytical pipeline in:\n",
    "\n",
    "- Shingleton, J., & Basiri, A. (2025). How close is \"close\"? An analysis of the spatial characteristics of perceived proximity using Large Language Models. AGILE GIScience Series, 6, 1‚Äì14. [10.1111/1755-2665.12276](https://doi.org/10.1111/1755-2665.12276)\n",
    "\n",
    "## üéØ Objectives of the Notebook\n",
    "\n",
    "1. Load **Inside Airbnb** listings for a city (London in the paper).\n",
    "2. Use a **Large Language Model (LLM)** to extract *explicitly named* places described as **near** each property.\n",
    "3. Robustly parse and clean the model output (incl. **JSON repair** and **fuzzy hallucination filtering**).\n",
    "4. Compute and interpret a **standard distance** for frequently mentioned (‚Äúreference‚Äù) locations.\n",
    "\n",
    "---\n",
    "\n",
    "## Working assumptions (for a classroom reproduction)\n",
    "\n",
    "- We run the LLM **locally** (via Ollama) and process a **small sample** (e.g. 200‚Äì500 listings).\n",
    "- We reproduce the *core logic* of the paper, not the full London-scale compute budget.\n",
    "\n",
    "> Keep notes as you go: any simplification you make is part of your reproducibility argument.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34238a9",
   "metadata": {},
   "source": [
    "## 0. Environment setup\n",
    "\n",
    "This exercise uses common data-science and geospatial libraries. If your environment is missing packages, install them below.\n",
    "\n",
    "**Libraries you will use**\n",
    "- `pandas` for tabular data\n",
    "- `geopandas` + `shapely` for geospatial data structures\n",
    "- `h3` for hexagonal indexing\n",
    "\n",
    "- `tqdm` for progress bars, because we are processing a lot of data\n",
    "\n",
    "- `ollama` for local LLM calls (or skip if you use another provider)\n",
    "- `json_repair` to handle malformed JSON from the LLM\n",
    "- `rapidfuzz` for fuzzy matching to remove hallucinated place names\n",
    "- `geopy` for geocoding with Nominatim\n",
    "- `osmnx` for downloading OpenStreetMap data\n",
    "- `plotly` for interactive maps & visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566971ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, uncomment and run:\n",
    "# %pip install -q pandas geopandas shapely h3 tqdm matplotlib ollama json_repair rapidfuzz geopy haversine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b52ba54",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "Read this cell carefully: in a reproducible notebook, you want *all imports in one place* so dependencies are obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b9609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import h3\n",
    "import ollama\n",
    "from rapidfuzz import fuzz\n",
    "from json_repair import repair_json\n",
    "\n",
    "import osmnx as ox\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# progress bar initialization\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7a96d2",
   "metadata": {},
   "source": [
    "## 2. Data: Inside Airbnb listings\n",
    "\n",
    "The paper uses Inside Airbnb listings for **London** and analyses property descriptions + coordinates. We will do the same analysis for Austria.\n",
    "\n",
    "For this exercise:\n",
    "- Open the [Inside Airbnb](https://insideairbnb.com/) website\n",
    "- Find a city in Austria. \n",
    "```python\n",
    "‚ùì `Is Graz Available?`\n",
    "```\n",
    "- Download `listings.csv.gz` from Inside Airbnb, then set `DATA_PATH` below.\n",
    "\n",
    "**Task (fill-in):** set `DATA_PATH` to your local file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f0a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (fill in): path to your downloaded listings.csv or listings.csv.gz\n",
    "DATA_PATH = ___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6274fa",
   "metadata": {},
   "source": [
    "## 3. Load and inspect the listings\n",
    "\n",
    "We will keep only the columns we need:\n",
    "- `id` (listing identifier)\n",
    "- `latitude`, `longitude`\n",
    "- `description` (the key text input to the LLM)\n",
    "- `neighborhood_overview` (for additional location context)\n",
    "- `neighbourhood_cleansed` (for later filtering)\n",
    "\n",
    "**üöÄ Tasks**\n",
    "1. How many total listings is present in the dataset?\n",
    "2. What does a typical description/neighborhood_overview look like?\n",
    "3. How many listings have missing descriptions and neighborhood_overview ? \n",
    "4. Create a kepler.gl map of all listings density based on h3 cells\n",
    "```python\n",
    "‚ùì `What is the coordinate reference system (CRS) of the data? Is it acceptable for our analysis?`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a4477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe \n",
    "raw_airbnb_df = pd.___(DATA_PATH)\n",
    "\n",
    "# TODO: print the number of rows of the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baf1fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only relevant columns\n",
    "use_cols = [\n",
    "    ___\n",
    "]\n",
    "\n",
    "airbnb_df = raw_airbnb_df[___].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e358df80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean na values\n",
    "# drop any row that has missing values in the columns we need\n",
    "airbnb_df = airbnb_df.dropna(subset=[___])\n",
    "\n",
    "# You can also use the parameter how='all' to drop only rows that have missing values in all columns\n",
    "# In the next step we will concatenate \"description\" and \"neighborhood_overview\"\n",
    "airbnb_df = airbnb_df.dropna(subset=[___], how='all')\n",
    "\n",
    "# TODO: print the number of of row in the cleaned sample of the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8dc0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: what look like the data? especially the description column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596fd53",
   "metadata": {},
   "source": [
    "## 4. Spatial aggregation with H3\n",
    "\n",
    "The paper aggregates properties into an H3 grid at **resolution 7** (~5.16 km¬≤ per hexagon in their setup).  \n",
    "We will use the same default so your maps are comparable in scale.\n",
    "\n",
    "**üöÄ Tasks** \n",
    "1. assign for each row a h3 cell id using the `point_to_h3` function\n",
    "2. group by h3 cell id and count the number of properties in each cell\n",
    "3. create a kepler.gl map of the h3 cell density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c67184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create a function that will be `apply` to your dataframe \n",
    "\n",
    "H3_RES = 7\n",
    "\n",
    "def latlon_to_h3(___) -> str:\n",
    "    \"\"\"Return the H3 cell id for a (lat, lon) coordinate.\"\"\"\n",
    "    return ___\n",
    "\n",
    "airbnb_df[\"h3_cell\"] = airbnb_df.apply(point_to_h3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1588c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: group your data by h3 cell and count the number of properties in each cell\n",
    "h3_airbnb_density = (\n",
    "    airbnb_df\n",
    "    .groupby(\"___\")\n",
    "    .___\n",
    "    .reset_index(name=\"airbnb_count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab47fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create a kepler.gl map of the h3 cell density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dcbd94",
   "metadata": {},
   "source": [
    "## 5. Filter your data to only include the inner city\n",
    "\n",
    "We have definitely too many points in our dataset to do the analysis. Let's filter to only include the inner city.\n",
    "\n",
    "**üöÄ Task:**\n",
    "1. Filter your data to only include the inner city\n",
    "2. Create a kepler.gl map of the inner city\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7356550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: keep only data points from the inner city\n",
    "# tip: use the `neighbourhood_cleansed` column, what are the possible values? \n",
    "\n",
    "# TODO: How many data points are left?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e6d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create a kepler.gl map of the inner city\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36386d99",
   "metadata": {},
   "source": [
    "## 6. Definition of the extraction prompt\n",
    "\n",
    "The paper uses a carefully engineered prompt (Appendix Fig. A1) to extract **explicitly named** nearby locations, excluding vague places and excluding the property‚Äôs own location.\n",
    "\n",
    "**üöÄ Tasks**\n",
    "1. Read the following prompt for Vienna, is it similar of the one you are used to write?\n",
    "2. Identify **three** design choices that reduce false positives\n",
    "3. What are the differences between this prompt and the one used in the paper?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c6718",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an Airbnb location extraction system for Vienna. \n",
    "Your task is to extract specific Points of Interest (POIs) that are mentioned as being NEAR the property.\n",
    "\n",
    "### 1. WHAT TO EXTRACT (Positive Criteria)\n",
    "You must only extract specific, physical landmarks that can be pinpointed on a map.\n",
    "- Specific Landmarks (e.g., \"Sch√∂nbrunn Palace\", \"St. Stephen's Cathedral\")\n",
    "- Named Squares/Streets (e.g., \"Stephansplatz\", \"Mariahilfer Stra√üe\", \"Florianigasse\")\n",
    "- Named Markets/Parks (e.g., \"Naschmarkt\", \"Prater\", \"Sch√∂nbornpark\")\n",
    "- Specific Transport Hubs (e.g., \"Westbahnhof\", \"U4 Pilgramgasse\")\n",
    "\n",
    "### 2. WHAT TO IGNORE (Negative Criteria)\n",
    "- IGNORE Districts and Quarters: Do NOT extract \"Neubau\", \"Favoriten\", \"1st District\", \"Freihausviertel\", \"Museumsquartier\" (unless referring to the specific complex, usually assume it's a district).\n",
    "- IGNORE Generic Amenities: Do NOT extract \"supermarkets\", \"restaurants\", \"bars\", \"shops\", \"metro station\" (if unnamed).\n",
    "- IGNORE Property Location: If the text says \"The apartment is located in [Place],\" ignore [Place]. Only extract places described as being *near*, *around*, or *walking distance* from the apartment.\n",
    "\n",
    "### 3. ADDRESS FORMATTING RULES\n",
    "- If a full address is in the text, use it.\n",
    "- If NO address is in the text, you MUST format the address as: \"[Name], Vienna\".\n",
    "- This is required formatting, not \"inventing.\"\n",
    "\n",
    "### 4. OUTPUT FORMAT\n",
    "Do not include markdown formatting or explanations.\n",
    "Keys: \"name\", \"address\", \"latitude\" (default null), \"longitude\" (default null), \"context\".\n",
    "\n",
    "### 5. FEW-SHOT EXAMPLES (Do not confuse these with your real task)\n",
    "\n",
    "Input: \"We are located in the heart of Neubau, close to the Zieglergasse station and a short walk to Mariahilfer Stra√üe.\"\n",
    "Output:\n",
    "[\n",
    "  {\n",
    "    \"name\": \"Zieglergasse station\",\n",
    "    \"address\": \"Zieglergasse station, Vienna\",\n",
    "    \"latitude\": null,\n",
    "    \"longitude\": null,\n",
    "    \"context\": \"close to the Zieglergasse station\"\n",
    "  },\n",
    "  {\n",
    "    \"name\": \"Mariahilfer Stra√üe\",\n",
    "    \"address\": \"Mariahilfer Stra√üe, Vienna\",\n",
    "    \"latitude\": null,\n",
    "    \"longitude\": null,\n",
    "    \"context\": \"short walk to Mariahilfer Stra√üe\"\n",
    "  }\n",
    "]\n",
    "\n",
    "Input: \"Enjoy the many shops and cafes in the 7th district. The bus takes you to the City Center.\"\n",
    "Output:\n",
    "[]\n",
    "\n",
    "Input: \"The apartment is located in the 3rd district, right next to the Belvedere Palace and just a short walk from the Rennweg station.\"\n",
    "Output:\n",
    "[\n",
    "  {\n",
    "    \"name\": \"Belvedere Palace\",\n",
    "    \"address\": \"Belvedere Palace, Vienna\",\n",
    "    \"latitude\": null,\n",
    "    \"longitude\": null,\n",
    "    \"context\": \"right next to the Belvedere Palace\"\n",
    "  },\n",
    "  {\n",
    "    \"name\": \"Rennweg station\",\n",
    "    \"address\": \"Rennweg station, Vienna\",\n",
    "    \"latitude\": null,\n",
    "    \"longitude\": null,\n",
    "    \"context\": \"short walk from the Rennweg station\"\n",
    "  }\n",
    "]\n",
    "Input: \"You are staying directly opposite the famous Hundertwasserhaus.\"\n",
    "Output:\n",
    "[\n",
    "  {\n",
    "    \"name\": \"Hundertwasserhaus\",\n",
    "    \"address\": \"Hundertwasserhaus, Vienna\",\n",
    "    \"latitude\": null,\n",
    "    \"longitude\": null,\n",
    "    \"context\": \"directly opposite the famous Hundertwasserhaus\"\n",
    "  }\n",
    "]\n",
    "\n",
    "### END OF EXAMPLES\n",
    "Now, analyze the following description and extract the locations based strictly on the text provided below.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d14d38",
   "metadata": {},
   "source": [
    "## 7. Calling an LLM locally (Ollama)\n",
    "\n",
    "We will call a local model via Ollama.\n",
    "- You need [Ollama](https://ollama.com/) running locally. Follow the link to download and install it.\n",
    "- You also need a model pulled (e.g., `llama3`, `qwen2`, etc.). You can find a list of available models [here](https://ollama.com/models).\n",
    "\n",
    "Running LLMs locally is extremely expensive for your RAM, Hard Drive and CPU. As a rule of thumb you need 1GB of RAM and 1GB of Hard Drive per 1B parameters. We will use a small model for this exercise: `qwen3:1.7b`.\n",
    "\n",
    "**üöÄ Task** \n",
    "1. set `MODEL_NAME` to a model you have locally\n",
    "2. Pull the model using `ollama.pull`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c6d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"qwen3:1.7b\" # You need 1.4GB of free space on your computer\n",
    "# ‚ö†Ô∏è You must have Ollama running locally \n",
    "\n",
    "ollama.pull(MODEL_NAME)\n",
    "\n",
    "# Create a client to call your model \n",
    "client = ollama.Client()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebeddf5",
   "metadata": {},
   "source": [
    "### 7.1 A minimal chat wrapper\n",
    "\n",
    "- We keep temperature = 0 for deterministic outputs (useful in reproduction).\n",
    "- We allow the model to think (set `think=True`), which is useful for debugging. Not all models support this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65362294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_extract_nearby(text: str, show_log: bool = True) -> str:\n",
    "    \"\"\"Return the raw model text output for one listing.\"\"\"\n",
    "\n",
    "    # define what you send to the model, the `system`` defines how the model should react, the `user` is the text you want to analyze \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    # you call the model here \n",
    "    resp = client.chat(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        options={\"temperature\": 0}, \n",
    "        think=True\n",
    "    )\n",
    "\n",
    "    model_text = resp[\"message\"][\"content\"]\n",
    "    thinking = resp[\"message\"].get(\"thinking\", \"...no thinking...\")\n",
    "\n",
    "    if show_log:\n",
    "        # print the model's thinking process\n",
    "        print(\"-----\")\n",
    "        print(text)\n",
    "        print(\"\\n thinking... \\n\")\n",
    "        print(\"response\",thinking)\n",
    "        print(\"\\n\\n =>\\n\")\n",
    "        print(model_text)\n",
    "    \n",
    "    # return only the content of the response without the thinking part \n",
    "    return model_text, thinking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c5618a",
   "metadata": {},
   "source": [
    "### 7.2 Try the model on one listing\n",
    "\n",
    "**üöÄ Task:** \n",
    "1. Run the model on one description of your dataframe \n",
    "‚ùì\n",
    "```python\n",
    "\n",
    "` - What is the thinking process of the model looking like?`\n",
    "` - Are the results correct?`\n",
    "` - How long does it take to run?`\n",
    "` - What happens if your rerun the same description? Is the output the same?`\n",
    "` - What happens if you change thinking to False?`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac964e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run your function on the first row of the dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7b2f77",
   "metadata": {},
   "source": [
    "## 8) Parsing the LLM output robustly\n",
    "\n",
    "In practice, models sometimes return malformed JSON. The paper repairs JSON first, and may ask the model to fix it if needed.\n",
    "\n",
    "We will implement a two-step parser:\n",
    "1. Try `json.loads`\n",
    "2. If that fails, try `json_repair.repair_json`, then parse again\n",
    "\n",
    "**üöÄ Tasks** \n",
    "1. creaze the function `parse_json_array` that clean the output of the LLM and try to parse it as a JSON array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0c959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_array(text: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Parse a JSON array from model output, with basic repair.\"\"\"\n",
    "    text = text.strip()\n",
    "\n",
    "    # 1) direct parse\n",
    "    # TODO: wrap the json load in a try-except block\n",
    "    data = json.loads(text)\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "\n",
    "    # 2) attempt repair\n",
    "    # TODO: use the library `json_repair` to repair the json\n",
    "    repaired = ___  # returns a JSON string\n",
    "    \n",
    "    # TODO: try to reload the repaired json\n",
    "\n",
    "    # 3) final fallback: return empty\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5978c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the output of your LLM from earlier to parse the JSON array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac1d62e",
   "metadata": {},
   "source": [
    "## 9) Batch extraction on a small sample\n",
    "\n",
    "We will:\n",
    "- run the LLM on each airbnb listing\n",
    "- parse the JSON\n",
    "- store results in a new column `nearby_raw`\n",
    "\n",
    "**üöÄ Task** \n",
    "1. ‚ö†Ô∏è Let select a very small sample (e.g. 3-10 rows) to test and debug your code\n",
    "2. Create a function that will be apply to each row of the dataframe to extract the locations mentioned in the description. The function should run your `ollama_extract_nearby` function and parse the JSON\n",
    "3. Apply the function to the dataframe and store the results in a new column `nearby_raw`\n",
    "4. Compute the mean number of extracted locations per listing\n",
    "5. Create a Kepler.gl map to visualize the extracted locations\n",
    "\n",
    "**üìù Note**\n",
    "- The execution might be long... so we will use `progress_apply` from `tqdm` to show a progress bar (instead of a simple `apply`)\n",
    "\n",
    "> **üì¢ TODO at home:**  \n",
    "> Run the operation on the entire dataframe and save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b2a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a small sample of the dataframe\n",
    "small_sample = airbnb_df.sample(___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7170b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that will be apply on your df\n",
    "def extract_for_row(row: str) -> pd.Series:\n",
    "    # define a variable to store the result\n",
    "    out = pd.Series({\n",
    "        \"locations\": [],\n",
    "        \"thinking\": None\n",
    "    })\n",
    "    # TODO: aggregate the columns to get the description and the neighborhood\n",
    "\n",
    "    # you can also clean the text\n",
    "    ___ = ___.replace(\"\\n\", \" \") # remove new lines\n",
    "    ___ = ___.replace(\"<br />\", \" \") # remove html tags\n",
    "    ___ = ___.trim().lower() # remove extra spaces and lower case\n",
    "\n",
    "    if len(___)==0: # make sure the text is not empty \n",
    "        return out\n",
    "    \n",
    "    # TODO: run your LLM function\n",
    "\n",
    "    # TODO: Return the parsed value\n",
    "    out[\"locations\"] = ___\n",
    "    out[\"thinking\"] = ___\n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f965d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_df[[\"nearby_raw\", \"thinking\"]] = ___.progress_apply(___, ___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d6139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save our results here if you ran all your dataframe \n",
    "\n",
    "# In json\n",
    "JSON_FILEPATH = ___\n",
    "airbnb_df.to_json(JSON_FILEPATH)\n",
    "\n",
    "# in parquet\n",
    "PARQUET_FILEPATH = ___\n",
    "airbnb_df.to_parquet(PARQUET_FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e87b41b",
   "metadata": {},
   "source": [
    "```python\n",
    "? \n",
    "` - What is the difference between parquet and json? check on your disk the space of each`\n",
    "` - Are the results similar when using the 1.7b model you ran this weekend?`\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc5a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate the number of locations per row and add it as a new column\n",
    "airbnb_df[\"nearby_raw_n\"] = ___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8cee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create a kepler map that show the average number of nearby locations per cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0991a",
   "metadata": {},
   "source": [
    "## 9. Reduce hallucinations with fuzzy presence checks\n",
    "\n",
    "A common failure mode is hallucinating a nearby place not mentioned in the text.\n",
    "\n",
    "\n",
    "We will:\n",
    "- use a fuzzy matching to remove hallucinated place names\n",
    "- create a function that check in a row if a location appears in the description text\n",
    "- store results in a new column `nearby_clean`\n",
    "\n",
    "\n",
    "**üöÄ Task** \n",
    "1. Set a threshold (start with 70)\n",
    "2. Compare how many locations remain after cleaning\n",
    "3. Compute the mean number of extracted locations per listing\n",
    "4. Create a Kepler.gl map to visualize the extracted locations\n",
    "\n",
    "**üìù Note**\n",
    "- You can load the parquet file `airbnb_df.parquet` to load the results of the previous step (a thinking model of 15b of parameters was run on the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef194650",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESH = 70  # experiment with 60, 70, 80\n",
    "\n",
    "def filter_dicts_by_presence(row):\n",
    "    # 1. Prepare the text to search in: create the text context you created in the previous step\n",
    "\n",
    "    kept_dicts = [] # init a dict for returning the filtered values \n",
    "\n",
    "    # 2. loop over your found locations\n",
    "    ____:\n",
    "        # 3. Check if the location name is in the text context\n",
    "        if fuzz.partial_ratio(___, ___) >= 70:\n",
    "            kept_dicts.append(___) # keep the item in the nearby column\n",
    "    return kept_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860a9d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply the above function to you dataframe\n",
    "airbnb_df['nearby_clean'] = ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47364054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comupte the new number of locations per row and add it as a new column & create a kepler map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d47be0",
   "metadata": {},
   "source": [
    "```python\n",
    "`? How many locations are left after cleaning?`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2809619",
   "metadata": {},
   "source": [
    "## 10. Adding missing coordinates to the cleaned locations\n",
    "\n",
    "```python\n",
    "‚ùì\n",
    "` - Have the lat and lon filled by the AI model?`\n",
    "` - What can we do to fix this?`   \n",
    "```\n",
    "\n",
    "We will:\n",
    "- use a geocoder to get the coordinates of the cleaned nearby locations: we will use Nominatim\n",
    "- create a function that geocodes the different nearby locations\n",
    "- drop the location that doesn't have any coordinates or that are not within Vienna\n",
    "- store results in a new column `nearby_coords`\n",
    "\n",
    "\n",
    "**üöÄ Task** \n",
    "1. Create a function that geocodes the different nearby locations\n",
    "2. Apply the function to the dataframe and store the results in a new column `nearby_coords`\n",
    "3. Drop the location that doesn't have any coordinates\n",
    "4. Compute the mean number of extracted locations per listing\n",
    "5. Create a Kepler.gl map to visualize the extracted locations\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e3fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Nominatim using geopy\n",
    "geolocator = Nominatim(user_agent=\"geoapiExercises\") # the user_agent can be anything you like\n",
    "# Nominatime enforce a limit rate of 1 request per second, you can simply use time.sleep(1) to enforce this or use RateLimiter from geopy\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c6fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define a function that geocodes the different nearby locations\n",
    "\n",
    "def geocode_locations(nearby_locs):\n",
    "    # loop ofber the nearby locations\n",
    "    ___:\n",
    "        # Call the geocoder on the address field\n",
    "        location = geolocator.geocode(___)\n",
    "\n",
    "        # TODO: Test iif you have a result before assigning a value\n",
    "        ___[\"latitude\"] = location.latitude\n",
    "        ___[\"longitude\"] = location.longitude\n",
    "    return nearby_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf090b",
   "metadata": {},
   "source": [
    "### 10.1 Try the geocoding on a few listings\n",
    "\n",
    "The process might be long because there is a sleep between each of your geocoding requests. Try to execute your function on several addresses. Then, you can load the file `airbnb_df_with_coords.parquet`. \n",
    "The data you'll load have been executed with Mapbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aef61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get a sample of your dataframe\n",
    "___\n",
    "\n",
    "\n",
    "# run the function on the sample\n",
    "sample['nearby_coords'] = sample['nearby_clean'].progress_apply(geocode_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99c642d",
   "metadata": {},
   "source": [
    "### 10.2 Drop the nearby addresses without coordinates or outside Vienna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e0deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get the geometry of Vienna using osmnx \n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42dbca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_coordinates(row, geometry):\n",
    "    # get bbox of vienna from the geometry\n",
    "    min_lon, min_lat, max_lon, max_lat = geometry.bounds\n",
    "\n",
    "    kept_dicts = [] # init a dict for returning the filtered values \n",
    "    \n",
    "    # TODO: create a loop over the nearby_coords\n",
    "    ___:\n",
    "        # TODO: Extract latitude and longitude of the dict\n",
    "        lon, lat = \n",
    "        if min_lon <= lon <= max_lon and min_lat <= lat <= max_lat:\n",
    "            kept_dicts.append(___)\n",
    "            \n",
    "    return kept_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_df['nearby_coords'] = airbnb_df.progress_apply( lambda row: ___(___, ___), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a9cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comupte the new number of locations per row and add it as a new column & create a kepler map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d95fe97",
   "metadata": {},
   "source": [
    "```python\n",
    "‚ùì\n",
    "`How many locations are left after cleaning?`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baac35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a histogram with plotly that shows the overview of the counts: raw number of addresses, non-hallucinate addresses, addresses with coordinates, addresses with coordinates inside Vienna \n",
    "\n",
    "___\n",
    "\n",
    "data = {\n",
    "    'Metric': ['Coords', 'Clear', 'Count'],\n",
    "    'Value': [coords, clear, count]\n",
    "}\n",
    "\n",
    "fig = px.bar(data, x='Metric', y='Value', title='Overview of the counts')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845f7b05",
   "metadata": {},
   "source": [
    "## 11. Reference locations and ‚Äúarea of influence‚Äù (standard distance)\n",
    "\n",
    "We will:\n",
    "- extend our dataframe to create a line per nearby location\n",
    "- display the most common nearby locations for the first district of Vienna \n",
    "- Calculate the respective distance to each of the airbnb listing\n",
    "- display the average distance on the h3 grid\n",
    "\n",
    "\n",
    "**üöÄ Task** \n",
    "1. keep only necessary columns\n",
    "2. explode the dataframe to create a line per nearby location\n",
    "3. group by the nearby location and count the number of Airbnb listings\n",
    "4. sort the results by the number of airbnb listings\n",
    "5. display the top 25 nearby locations\n",
    "6. calculate the distance to each of the airbnb listing\n",
    "7. display the average distance on the h3 grid\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca8cf77",
   "metadata": {},
   "source": [
    "#### 11.1 Exploring the most common locations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: drop the columns that are not necessary\n",
    "# We need the nearby_coords, cell_id, latitude, longitude and id\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e37bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: explode the df\n",
    "airbnb_df_exploded = airbnb_df.explode('___', ignore_index=True).dropna(subset=[\"___\"]).reset_index(drop=True).copy()\n",
    "\n",
    "# extract the location name and coordinates for each dict\n",
    "airbnb_df_exploded['location_name'] = airbnb_df_exploded['nearby_coords'].apply(lambda x: x['name'])\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f9ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: display the 25 most common nearby location\n",
    "___ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0f1761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use plotly to create a histogram\n",
    "fig = px.histogram(x=value_counts.index, \n",
    "    y=value_counts.values,\n",
    "    labels={'x': 'Location Name', 'y': 'Count'}, height=400)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3bc43b",
   "metadata": {},
   "source": [
    "```python\n",
    "‚ùì \n",
    "`- What do you notice from the names, is there any issue? `\n",
    "`- When you look at a ‚Äúhigh influence‚Äù location, what splatial factors might explain it beyond geometry? `\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71a573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) Regroup similar names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ec59d8",
   "metadata": {},
   "source": [
    "### 11.2 Computing the relative distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ff3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "EARTH_RADIUS_KM = 6371.0088\n",
    "\n",
    "def haversine_km(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n",
    "    \"\"\"Great-circle distance (km) between two WGS84 points.\"\"\"\n",
    "    # TODO (fill in): implement haversine\n",
    "    phi1 = math.radians(lat1)\n",
    "    phi2 = math.radians(lat2)\n",
    "    dphi = math.radians(lat2 - lat1)\n",
    "    dlambda = math.radians(lon2 - lon1)\n",
    "\n",
    "    a = math.sin(dphi/2)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dlambda/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    return EARTH_RADIUS_KM * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e792e14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_df_exploded['distance'] = airbnb_df_exploded.apply(lambda row: ___, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbed8360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Aggregate the data by h3 cell and calculate the mean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a721b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add the results to a Kepler map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245fe6a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Key limitations \n",
    "- **Sampling bias**: Airbnb listings represent a particular demographic and purpose.\n",
    "- **Coordinate uncertainty**: listing coordinates are approximate.\n",
    "- **Model bias**: small models may miss local place names; larger models may hallucinate more.\n",
    "- **Geocoding uncertainty**: ambiguous names and API limits.\n",
    "\n",
    "\n",
    "#### Reflection questions \n",
    "1. Which step contributes the largest uncertainty to the final maps: prompt design, model choice, JSON repair, hallucination filtering, or geocoding?\n",
    "2. How would you validate extraction quality without manually labelling thousands of listings?\n",
    "3. What ethical risks exist when mining user-generated text for spatial inference?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b377a7",
   "metadata": {},
   "source": [
    "**Congratulations! üéâ You have successfully completed the exercise.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
