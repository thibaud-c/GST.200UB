{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8333a1",
   "metadata": {},
   "source": [
    "# Crime Hotspot Mapping and Prediction Accuracy Index (PAI)\n",
    "\n",
    "\n",
    "> Chainey, S., Tompson, L., & Uhlig, S. (2008b). The utility of hotspot mapping for predicting spatial patterns of crime. Security Journal, 21(1‚Äì2), 4‚Äì28. [10.1057/palgrave.sj.8350066](https://doi.org/10.1057/palgrave.sj.8350066)\n",
    "\n",
    "\n",
    "## üéØ Objectives\n",
    "\n",
    "- Explore a real crime data set with Python and GeoPandas.  \n",
    "- Create different types of **hotspot maps** (districts, beats, hexagonal grid, and clusters).  \n",
    "- Implement and interpret the **Prediction Accuracy Index (PAI)** as proposed in crime analysis research (Chainey, Tompson & Uhlig, 2008).  \n",
    "- Reflect on how different hotspot methods perform at predicting where crime will occur next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7531085",
   "metadata": {},
   "source": [
    "### Before you start\n",
    "\n",
    "Answer these short questions in your own words (just a sentence each):\n",
    "\n",
    "1. What do you think a **crime hotspot** is?  \n",
    "2. Why might police care about **predicting** where crime will happen, rather than only mapping where it already happened?  \n",
    "3. What is one potential **risk** of basing decisions on hotspot maps?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0113f3",
   "metadata": {},
   "source": [
    "## Part 0: Setup\n",
    "\n",
    "- (Optionally) install missing packages.  \n",
    "- Import the Python libraries used throughout the notebook.  \n",
    "- Define some **coordinate reference systems (CRS)** and file paths.\n",
    "\n",
    "### Data\n",
    "\n",
    "For reproducibility, use an extract of the **Chicago ‚ÄúCrimes ‚Äì 2001 to Present‚Äù** dataset\n",
    "from the City of Chicago Open Data Portal.\n",
    "\n",
    "1. Open the City of Chicago Open Data Portal.  \n",
    "2. Search for **[Crimes ‚Äì 2001 to Present](https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-Present/ijzp-q8t2/about_data)**.  \n",
    "3. Filter (for example) to the year 2025 and export as CSV.  \n",
    "4. Save it locally as: `data/chicago_crimes_sample.csv`.\n",
    "5. Download also the districts and the beats of Chicago\n",
    "\n",
    "> You may choose a different filename or time period ‚Äì just remember to update the path\n",
    "> and filters in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d279b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: install missing libraries in your environment\n",
    "# Run this cell **only if** you get ImportError messages below.\n",
    "# Remove the leading `#` before %pip to actually install.\n",
    "\n",
    "# %pip install pandas geopandas shapely numpy matplotlib plotly keplergl h3 scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1074f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data handling\n",
    "import pandas as pd        \n",
    "import numpy as np         # numerical operations and arrays\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# Visualisation\n",
    "from keplergl import KeplerGl  \n",
    "\n",
    "# Spatial analysis\n",
    "import h3 # Hexagonal grid indexing\n",
    "from sklearn.cluster import DBSCAN # Clustering for hotspot detection\n",
    "\n",
    "# Spatial statistics (need to install pysal)\n",
    "import pysal.lib as ps # PySAL \n",
    "from pysal.explore import esda # Moran's I, Local Moran's I, and Local Getis-Ord G\n",
    "from splot.esda import moran_scatterplot # visualize moran\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694e7cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "\n",
    "# Global CRS settings\n",
    "GLOBAL_CRS = \"EPSG:4326\"   # WGS84 geographic coordinates (lat/lon)\n",
    "METRIC_CRS = \"EPSG:3857\"   # Web Mercator (units ~ metres) -> you can also use EPSG:2163 for a projected US Albers\n",
    "\n",
    "# File paths (adapt if your files are in a subfolder like './data/...')\n",
    "CRIMES_DATA_PATH = ___\n",
    "DISTRICT_DATA_PATH = ___\n",
    "BEATS_DATA_PATH = ___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dfcf5d",
   "metadata": {},
   "source": [
    "## Part 1: Density and PAI estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b35c76",
   "metadata": {},
   "source": [
    "### A. Load crime data and first exploration\n",
    "\n",
    "We start by:\n",
    "\n",
    "1. Loading the crime CSV file into a `pandas.DataFrame`.  \n",
    "2. Dropping clearly redundant coordinate columns (if present).  \n",
    "3. Removing records with missing coordinates.  \n",
    "4. Converting the table to a `GeoDataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f7bab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load crime data\n",
    "crimes = pd.read_csv(CRIMES_DATA_PATH)\n",
    "\n",
    "print(\"Original number of rows:\", ___)\n",
    "print(\"Columns:\", list(___))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc4775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant coordinate columns if present\n",
    "cols_to_drop = [___]\n",
    "df_crimes = ___.drop(columns=___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f55dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing lat/lon\n",
    "number_of_rows_before = ___\n",
    "df_crimes = df_crimes.dropna(subset=[___])\n",
    "number_of_rows_after\n",
    "print(\"Missing coordinates (%):\", ___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6584267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick descriptive statistics\n",
    "df_crimes.___()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf63df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the interactive visualization with Kepler\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c0a6ce",
   "metadata": {},
   "source": [
    "> ‚ÅâÔ∏è How might **data quality issues** (missing coordinates, mis-typed districts, wrong timestamps) influence your hotspot analysis and any decisions based on it?\n",
    ">\n",
    "> ‚ÅâÔ∏è After exploring the map: do you see any **clearly visible clusters** or corridors of crime?  \n",
    ">\n",
    "> ‚ÅâÔ∏è How might the **underlying urban structure** (roads, land use, transport) explain what you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2de4c79",
   "metadata": {},
   "source": [
    "### B. Train‚Äìtest split: preparing for prediction evaluation\n",
    "\n",
    "To evaluate hotspot maps as **predictors**, we follow the general idea from Chainey et al. (2008):\n",
    "\n",
    "- Use **historical crime data** (training period) to build hotspot maps.  \n",
    "- Use **future crime data** (test period) to evaluate how many test crimes fall inside the hotspots.\n",
    "\n",
    "Here we use a **simple split**:\n",
    "\n",
    "- Training set: all crimes **before** the last 7 days.  \n",
    "- Test set: crimes in the **last 7 days** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6defe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Date' is a proper datetime column\n",
    "df_crimes[\"Date\"] = pd.to_datetime(___)\n",
    "\n",
    "max_date = ___\n",
    "last_week_start_date = max_date - pd.Timedelta(days=7)\n",
    "\n",
    "# To split out the dataframe, we will use a mask. A mask is a boolean condition (test)\n",
    "\n",
    "mask_test = df_crimes[\"Date\"] > last_week_start_date # here we want to filter all the columns that have a date > to the last_week_start_date\n",
    "\n",
    "df_test = df_crimes.loc[mask_test].copy()\n",
    "df_train = df_crimes.loc[___].copy() # here we want the opposite of our test mask. In pandas you can use `~` to get the opposite of a boolean condition \n",
    "\n",
    "print(f\"Training set: {___} crimes\")\n",
    "print(f\"Test set: {___} crimes\")\n",
    "\n",
    "print(\"Training period:\", ___, \"to\", ___)\n",
    "print(\"Test period:\", ___, \"to\", ___)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296d1540",
   "metadata": {},
   "source": [
    "### C. Loading district and beat boundaries\n",
    "\n",
    "We now load the polygon boundaries for:\n",
    "\n",
    "- **Districts** (larger administrative areas)  \n",
    "- **Beats** (smaller operational policing units)\n",
    "\n",
    "Both are stored as CSVs with a WKT geometry column called `the_geom`.\n",
    "\n",
    "---\n",
    "\n",
    "##### üìö Concept: administrative units and the MAUP\n",
    "\n",
    "Administrative units (districts, beats) are **not neutral**:\n",
    "\n",
    "- Their size and shape are products of history, politics, and operational needs.  \n",
    "- Changing the boundaries can change the **appearance** of a hotspot map.  \n",
    "- This relates to the **modifiable areal unit problem (MAUP)** ‚Äì results can change when you change the zoning or aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2607ef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load districts\n",
    "df_districts = ___\n",
    "\n",
    "# Convert WKT to geometry\n",
    "df_districts[\"geometry\"] = df_districts[\"the_geom\"].apply(wkt.loads)\n",
    "gdf_districts = gpd.GeoDataFrame(___, geometry=___, crs=___)\n",
    "\n",
    "# A cool addition to make you work on your conflict resolution\n",
    "\n",
    "print(\"Number of Districts:\", ___)\n",
    "\n",
    "# Quick plot\n",
    "gdf_districts.___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af800f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load beats\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b80d39",
   "metadata": {},
   "source": [
    "### D. Thematic mapping of geographic boundary areas\n",
    "\n",
    "One common hotspot method is to **count crimes per administrative unit** and shade polygons by that count.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Aggregate crimes to **districts** and map them.  \n",
    "2. Aggregate crimes to **beats** and map them.\n",
    "\n",
    "This corresponds to ‚Äúthematic mapping of geographic boundary areas‚Äù in the paper.\n",
    "\n",
    "--- \n",
    "\n",
    "##### üìö Concept: choropleth maps\n",
    "\n",
    "A **choropleth map** colours polygons based on a value (here: crime count).  \n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Very simple and widely understood.  \n",
    "- Easy to compute and update regularly.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Sensitive to how boundaries are drawn (MAUP).  \n",
    "- Does not show within-unit variation.\n",
    "- Large polygons with small populations can look very \"hot\".  \n",
    "\n",
    "Keep these pros and cons in mind when interpreting your maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e5b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate training crimes by district\n",
    "crime_dist_counts = ( # we write everything in parentheses to make it more readable and write over several lines\n",
    "    ___ # training data\n",
    "    .groupby(___) # group by district\n",
    "    .size() # count crimes per district\n",
    "    .reset_index(name=\"crimes_count\") # reset index and name the count column\n",
    ")\n",
    "\n",
    "# What does the aggregated data look like?\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbb05a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join to district polygons \n",
    "gdf_districts_with_crimes = ___.merge(\n",
    "    ___,\n",
    "    how=\"left\",\n",
    "    left_on=\"___\",\n",
    "    right_on=\"___\"\n",
    ")\n",
    "\n",
    "# Replace missing counts (districts with no crimes) with 0\n",
    "gdf_districts_with_crimes[\"crimes_count\"] = gdf_districts_with_crimes[\"crimes_count\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c418b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's show the result\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefe833a",
   "metadata": {},
   "source": [
    "> ‚ÅâÔ∏è Which districts appear as the most intense hotspots?  \n",
    ">\n",
    "> ‚ÅâÔ∏è Do these districts cover **large areas** or relatively small ones?  \n",
    ">\n",
    "> ‚ÅâÔ∏è How might this affect resource allocation decisions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36f7eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same operation with the beats (it is the one we will use to calculate the PAI)\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74eeb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üí° You can already add 2 layers in your kepler map: \n",
    "# 1. The beats layer with their crime counts\n",
    "# 2. The point test layer\n",
    "# Try to visualize both layers at the same time!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defacecf",
   "metadata": {},
   "source": [
    "> ‚ÅâÔ∏è Do the same areas appear as hotspots at both scales?  \n",
    ">\n",
    "> ‚ÅâÔ∏è Where do the two maps **disagree**?  \n",
    ">\n",
    "> ‚ÅâÔ∏è Which map do you find more **actionable** for operational policing, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62183594",
   "metadata": {},
   "source": [
    "### E. Grid / hexagonal hotspot mapping with H3\n",
    "\n",
    "Another common approach is to impose a **regular grid** over the city and count crimes in each cell.\n",
    "\n",
    "We will use:\n",
    "\n",
    "- The **H3 hexagonal grid system** (https://h3geo.org)  \n",
    "- Each crime point will be assigned to a hexagon.  \n",
    "- We will then map the count of crimes per hexagon.\n",
    "\n",
    "---\n",
    "\n",
    "##### üìö Concept: what is H3?\n",
    "\n",
    "H3 is a **global hierarchical hexagonal grid system** originally developed at Uber. Key ideas:\n",
    "\n",
    "- The world is divided into **hexagonal cells** at different resolutions.  \n",
    "- Each cell has a unique **index** (e.g. `\"882a1072b9fffff\"`).  \n",
    "- Higher resolutions ‚Üí smaller hexagons (finer detail).  \n",
    "- Neighbouring cells have similar size and shape, which avoids some biases from irregular polygons.\n",
    "\n",
    "Why hexagons?\n",
    "\n",
    "- They have more **neighbours** than squares (6 vs 4), reducing directional bias.  \n",
    "- They cover space more smoothly than many other shapes.  \n",
    "- They are popular in spatial statistics and ecological modelling.\n",
    "\n",
    "In hotspot analysis, H3 offers:\n",
    "\n",
    "- A **neutral, regular** spatial unit (not tied to administrative boundaries).  \n",
    "- Easy multi-scale analysis by changing the resolution.\n",
    "\n",
    "We will map crimes to hexagons at a single resolution as a first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91abc6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "H3_RESOLUTION = 8  # smaller number = bigger hexagons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a32484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will define 2 functions: one for assigning a hexagon to each geometry, and one to extract the geometry of the hexagons\n",
    "def get_hex_id(geometry, resolution=H3_RESOLUTION):\n",
    "    \"\"\"Assign a hexagon ID to a geometry.\"\"\"\n",
    "    if geometry is None or geometry.is_empty:\n",
    "        return None\n",
    "    return h3.latlng_to_cell(geometry.___, geometry.___, resolution)\n",
    "\n",
    "\n",
    "def get_hex_geometry(hex_id):\n",
    "    \"\"\"Get the geometry of a hexagon from its ID.\"\"\"\n",
    "\n",
    "    boundary = h3.cell_to_boundary(hex_id) # unfortunately, h3 returns a list of reverted coordinates and not a geometry\n",
    "\n",
    "    # swap to (lon, lat) for shapely\n",
    "    coords = [(lon, lat) for lat, lon in boundary]\n",
    "    \n",
    "    return Polygon(coords)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f54d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of your dataframe to keep the original clean\n",
    "df_h3_train = gdf_train.copy()\n",
    "\n",
    "# transform our df in a GeoDataFrame \n",
    "gdf_h3_train = gpd.GeoDataFrame(___, geometry=gpd.points_from_xy(___), crs=___)\n",
    "\n",
    "# assign a hex_id in each row based on your geometry\n",
    "\n",
    "gdf_h3_train[\"h3_cell\"] = gdf_h3_train[___].apply(___)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df4f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group and Count crimes per cell\n",
    "h3_crimes = (\n",
    "    ___\n",
    ") # we first group the hex to optimize the calculation of the geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f631dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build hexagon geometries\n",
    "h3_crimes[\"geometry\"] = h3_crimes[___].apply(___)\n",
    "\n",
    "# recreate a geodataframe\n",
    "gdf_h3_crimes = ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb7684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's show the result (h3 grid and points from test layer)\n",
    "___\n",
    "\n",
    "# What geometry is taken by default by Kepler? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12d4b65",
   "metadata": {},
   "source": [
    "> ‚ÅâÔ∏è Visually, do the last-week crimes tend to fall inside or outside the **densest hexagons**?  \n",
    ">\n",
    "> ‚ÅâÔ∏è How does your visual impression compare with the PAI values you will compute later?\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbbd72b",
   "metadata": {},
   "source": [
    "### F. DBSCAN clustering\n",
    "\n",
    "As a simple unsupervised method, we can use **DBSCAN** to find clusters of crime points.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Take a random sample of training crimes (to keep computation light).  \n",
    "2. Project to a metric CRS (so distances are in metres).  \n",
    "3. Run DBSCAN with an `eps` value in metres.  \n",
    "4. Visualise the clusters.\n",
    "\n",
    "---\n",
    "\n",
    "##### üìö Concept: what is DBSCAN?\n",
    "\n",
    "DBSCAN (**D**ensity-**B**ased **S**patial **C**lustering of **A**pplications with **N**oise) is a clustering algorithm with two key parameters:\n",
    "\n",
    "- **`eps`** ‚Äì radius of the neighbourhood (here, in metres).  \n",
    "- **`min_samples`** ‚Äì minimum number of points required to form a dense region.\n",
    "\n",
    "The algorithm:\n",
    "\n",
    "1. For each point, count how many neighbours it has within distance `eps`.  \n",
    "2. If it has at least `min_samples` neighbours, it is a **core point**.  \n",
    "3. Clusters are formed by connecting core points and their nearby neighbours.  \n",
    "4. Points that are not part of any cluster are labelled as **noise** (`-1`).\n",
    "\n",
    "Why DBSCAN is useful for crime analysis:\n",
    "\n",
    "- It discovers clusters of **arbitrary shape** (not just circular).  \n",
    "- It can identify **noise** (isolated incidents).  \n",
    "- You do not need to choose the **number of clusters** in advance.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "- Results are sensitive to the choice of `eps` and `min_samples`.  \n",
    "- A single pair of parameters may not work well across all areas (dense city centre vs sparse suburbs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce96322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN can be very heavy on computation for large datasets\n",
    "# Let's reduce our dataset to make it manageable\n",
    "\n",
    "# Extract a subsample of our dataframe\n",
    "dbscan_crimes = gdf_test.sample(frac=0.1, random_state=42) # frac=0.1 takes 10% of the data, random_state ensures reproducibility\n",
    "\n",
    "# Project to a metric CRS for distance calculations\n",
    "___\n",
    "\n",
    "# DBSCAN takes a set/stack of coordinates (from numpy) and not a dataframe. \n",
    "# Extract coodinates of our sample\n",
    "coords = np.array( # create a numpy array\n",
    "            list( # convert to list\n",
    "                zip( # zip x and y coordinates\n",
    "                    ___, \n",
    "                    ___\n",
    "                )\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da837bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We imported sklearn that provide a plug and play solution to run a DBSCAN\n",
    "db = DBSCAN(\n",
    "    eps=500,          # neighbourhood radius in metres, try different values \n",
    "    min_samples=50,   # minimum points to form a cluster, try different values\n",
    "    n_jobs=-1,        # use all available cores\n",
    "    metric=\"euclidean\" # distance metric to use\n",
    ")\n",
    "\n",
    "# Fit the DBSCAN model\n",
    "labels = db.fit_predict(coords)\n",
    "\n",
    "# Add cluster labels to the sample dataframe\n",
    "___\n",
    "\n",
    "print(\"Unique cluster IDs (‚àí1 = noise):\", ___.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c33aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's show the result (DBSCAN and points from test layer)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90e8cf",
   "metadata": {},
   "source": [
    "> ‚ÅâÔ∏è What happens if you **increase** `eps` while keeping `min_samples` constant?  \n",
    ">\n",
    "> ‚ÅâÔ∏è What happens if you **decrease** `min_samples` while keeping `eps` constant?  \n",
    ">\n",
    "> ‚ÅâÔ∏è How do your identified clusters compare to the choropleth maps or the H3 hexagon areas you created earlier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bee5b0",
   "metadata": {},
   "source": [
    "### G. Prediction Accuracy Index (PAI)\n",
    "\n",
    "We now want to **evaluate** how good our hotspot maps are at predicting where crime will occur next.\n",
    "\n",
    "Following Chainey et al. (2008), we use the **Prediction Accuracy Index (PAI)**:\n",
    "\n",
    "\n",
    "PAI = ( n / N ) / ( a / A )\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "- n = number of **test crimes** that fall inside hotspot areas  \n",
    "- N = total number of test crimes in the study area  \n",
    "- a = area of hotspots (e.g. total area of selected hexagons / beats / clusters)  \n",
    "- A = total area of the study area\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- **PAI = 1** ‚Üí hotspot performs like random selection.  \n",
    "- **PAI > 1** ‚Üí hotspot is better than random (good).  \n",
    "- **Higher PAI** ‚Üí more crimes captured in a smaller area.  \n",
    "\n",
    "We will:\n",
    "\n",
    "1. Compute the **study area** (union of all districts).  \n",
    "2. Turn the test-period crimes into a GeoDataFrame in the metric CRS.  \n",
    "3. Define a helper function to compute PAI.  \n",
    "4. Apply it to:\n",
    "   - Top 15% beats by crime count  \n",
    "   - Top 15% hexagons by crime count  \n",
    "   - DBSCAN clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8210ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total study area (in metric CRS) -> we can use an aggregation of the districts\n",
    "# first to calculate a metric value we need to change the CRS of our dataframe\n",
    "gdf_districts_metric = ___\n",
    "study_area = gdf_districts_metric.geometry.___\n",
    "\n",
    "\n",
    "print(f\"Total study area: {study_area/1e6:.2f} km¬≤\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e0eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our test df in gdf (and change the crs)\n",
    "___\n",
    "\n",
    "# count the total number of crimes in our test set\n",
    "total_test_crimes = ___\n",
    "\n",
    "print(f\"Total number of test crimes: {total_test_crimes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bb0f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a helper function to compute PAI\n",
    "def compute_pai(hits, hotspot_area, total_test_crimes, study_area):\n",
    "    \"\"\"Compute Prediction Accuracy Index (PAI) for a given set of hotspot polygons.\n",
    "\n",
    "    hits: number of test crimes within hotspots\n",
    "    hotspot_area: total area of hotspot polygons in m¬≤\n",
    "    total_test_crimes: total number of test crimes\n",
    "    study_area: total area of study region (in m¬≤)\n",
    "    \"\"\"\n",
    "\n",
    "    # Hit rate (n / N)\n",
    "    hit_rate = hits / total_test_crimes\n",
    "\n",
    "    # Area percentage (a / A)\n",
    "    area_pct = hotspot_area / study_area\n",
    "\n",
    "    pai = (hit_rate / area_pct) if area_pct > 0 else np.nan\n",
    "\n",
    "    return pai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cf2166",
   "metadata": {},
   "source": [
    "#### G.1 PAI for beat-based hotspots\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Normalise crime counts by the maximum.  \n",
    "2. Select the **top 15%** beats.  \n",
    "3. Compute PAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bb7121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy to avoid modifying earlier GeoDataFrame\n",
    "beats_hotspots = gdf_beats_with_crimes.copy()\n",
    "\n",
    "# Normalize crime counts by maximum\n",
    "beats_hotspots[\"norm_crime\"] = ___\n",
    "\n",
    "# Top 15% beats by crime intensity\n",
    "threshold_beat = beats_hotspots[\"norm_crime\"].quantile(0.85) # you can get the value at a specific quantile with .quantile()\n",
    "# Create a mask to select top beats\n",
    "mask_top_beat = ___\n",
    "top_beats = beats_hotspots.loc[mask_top_beat].copy()\n",
    "\n",
    "print(\"Number of beat hotspots:\", ___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d2ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute area of top beats\n",
    "# change crs to metric\n",
    "top_beats = ___\n",
    "top_beats_area = top_beats.geometry.___\n",
    "\n",
    "print(f\"Total area of top beats: {top_beats_area/1e6:.2f} km¬≤\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74662342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the hits within the top beats area\n",
    "beat_hits = gdf_test_metric.within(top_beats.geometry.unary_union).sum() # .unary_union combines all geometries into one, within() checks if points are within the union\n",
    "\n",
    "print(f\"Hits within top beats: {___}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c3a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PAI\n",
    "pai_beats = compute_pai(____)\n",
    "\n",
    "print(f\"BEATS PAI: {___:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5354a6de",
   "metadata": {},
   "source": [
    "#### G.2 PAI for H3 hexagon hotspots\n",
    "\n",
    "Similar to the beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f71bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the top h3 hotspots by area\n",
    "# identify and extract the top h3\n",
    "\n",
    "top_h3 = ___\n",
    "\n",
    "print(\"Number of h3 hotspots:\", ___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ce1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the area of the top h3 hotspots\n",
    "\n",
    "top_h3_area = ___\n",
    "\n",
    "print(f\"Total area of top beats: {top_h3_area/1e6:.2f} km¬≤\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5c1b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the hits within the top h3 area\n",
    "\n",
    "h3_hits = ___ \n",
    "\n",
    "print(f\"Hits within top h3: {___}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aa4e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PAI\n",
    "pai_h3 = compute_pai(____)\n",
    "\n",
    "print(f\"H3 PAI: {___:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c69ce78",
   "metadata": {},
   "source": [
    "#### G.3 PAI for DBSCAN clusters\n",
    "\n",
    "To evaluate DBSCAN clusters as hotspots, we:\n",
    "\n",
    "1. Construct one polygon per cluster (convex hull).  \n",
    "2. Treat these polygons as hotspot areas.  \n",
    "3. Compute PAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ff978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the cluster geometry \n",
    "\n",
    "clusters = []\n",
    "\n",
    "for cid, group in dbscan_crimes.groupby(\"cluster_id\"): # Loop over each cluster \n",
    "    # cid is the cluster ID\n",
    "    # group is the set of crimes belonging to that cluster\n",
    "    \n",
    "    if cid == -1:\n",
    "        continue  # skip noise\n",
    "\n",
    "    hull = group.unary_union.convex_hull  # create one polygon per cluster\n",
    "\n",
    "    clusters.append({\n",
    "        \"cluster_id\": ___,\n",
    "        \"n_points\": ___,\n",
    "        \"geometry\": ___\n",
    "    })\n",
    "\n",
    "clusters_gdf = gpd.GeoDataFrame(___, geometry=___, crs=___)\n",
    "\n",
    "top_clusters = ___\n",
    "print(\"Number of hot points:\", top_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c0d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the area of the top cluster hotspots\n",
    "\n",
    "top_cluster_area = ___\n",
    "print(f\"Hotspot area: {top_cluster_area/1e6:.2f} km¬≤\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd30554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the hits within the top h3 area\n",
    "\n",
    "cluster_hits = ___ \n",
    "\n",
    "print(f\"Hits within top h3: {___}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2686afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PAI\n",
    "pai_cluster = compute_pai(____)\n",
    "\n",
    "print(f\"H3 PAI: {___:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0b050f",
   "metadata": {},
   "source": [
    "## PART 2: Basic exploratory data analysis (EDA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714f7d51",
   "metadata": {},
   "source": [
    "### A. When is crime most frequent? (Temporal pattern by month)\n",
    "\n",
    "üí° **Tips**: extracting the month\n",
    "- Use the `pd.to_datetime` function to convert the `Date` column to a datetime object.\n",
    "- Use the `dt.month` attribute to extract the month from the datetime object.\n",
    "- Use the `value_counts` function to count the number of crimes per month.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d2fd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of your crimes data\n",
    "eda_crimes = ___\n",
    "\n",
    "# Extract month from date\n",
    "eda_crimes['month'] = ___\n",
    "\n",
    "# create a plotly bar chart by counting crimes per month with .value_counts()\n",
    "px.bar(eda_crimes['month']___)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc5e03c",
   "metadata": {},
   "source": [
    "> ‚ÅâÔ∏è Which months show the highest and lowest crime counts?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb8b371",
   "metadata": {},
   "source": [
    "### B. What are the main crime types? (Categorical distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20b81db",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_counts = ___.value_counts()\n",
    "\n",
    "# extract the 20 top crime categories and their counts \n",
    "top_20 = ___.head(___)\n",
    "\n",
    "px.bar(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b7a3fb",
   "metadata": {},
   "source": [
    "> ‚ÅâÔ∏è Which crime types dominate the dataset?\n",
    ">\n",
    "> ‚ÅâÔ∏èAre there any crime types you expected to see but are rare or missing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62464d6",
   "metadata": {},
   "source": [
    "### C. Grouping detailed crime types into broader categories\n",
    "\n",
    "Right now we have many detailed crime types (e.g. ‚ÄúTHEFT‚Äù, ‚ÄúBURGLARY‚Äù, ‚ÄúNARCOTICS‚Äù).\n",
    "For some analyzes, it is helpful to group them into broader categories. Additionally, sometimes you have some categories that need to be translated in other categories.\n",
    "\n",
    "üí° **Tips**: Mapping\n",
    "- Create a Python dictionary mapping detailed types (broader category label or new categories)\n",
    "- Use .map() to apply this mapping to the Primary Type column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a084ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping categories\n",
    "\n",
    "# create mapping of crime types to main categories (keeping only the 12 most important crime types)\n",
    "main_categories = {\n",
    "    # \"old category\": \"new category\"\n",
    "    # Violence Against Persons\n",
    "    \"ASSAULT\": \"violence_against_person\",\n",
    "    \"BATTERY\": \"violence_against_person\",\n",
    "    # Residential Burglary\n",
    "    \"BURGLARY\": \"burglary\",\n",
    "    # Thefts\n",
    "    \"THEFT\": \"theft\",  \n",
    "    \"MOTOR VEHICLE THEFT\": \"theft\",    \n",
    "    \"DECEPTIVE PRACTICE\": \"theft\",\n",
    "    \"ROBBERY\": \"theft\",\n",
    "    # Drugs\n",
    "    \"NARCOTICS\": \"drug_offense\",\n",
    "    # Property Environmental/Damage\n",
    "    \"CRIMINAL DAMAGE\": \"prop_env_damage\",\n",
    "    \"CRIMINAL TRESPASS\": \"prop_env_damage\",\n",
    "    # Other\n",
    "    \"OTHER OFFENSE\": \"Other\",\n",
    "    \"WEAPONS VIOLATION\": \"Other\"\n",
    "}\n",
    "\n",
    "# Only create main_category with the mapping\n",
    "eda_crimes[___] = eda_crimes[___].map(___).fillna(\"non_assigned\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9025d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many non-assigned values do you have?\n",
    "___\n",
    "\n",
    "# drop all non-assigned values (üí° you can create a mask)\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e420546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize counts of the broad main categories\n",
    "px.bar(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530e04c8",
   "metadata": {},
   "source": [
    "> ‚ÅâÔ∏è What are the pros and cons of working with fewer, broader categories instead of detailed types?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8feec8",
   "metadata": {},
   "source": [
    "### D. How do crime categories shift in space over time? (Mean centres)\n",
    "\n",
    "##### üìö Concept: Mean centres (spatial average)\n",
    "\n",
    "A **mean centre** is the geographic equivalent of the *average* in 1D statistics.\n",
    "\n",
    "- In ordinary statistics you might compute the **mean value** of a list of numbers.  \n",
    "- In spatial analysis, we can compute the **mean x-coordinate** and **mean y-coordinate** of a set of points.\n",
    "\n",
    "If each crime event has coordinates x_i, y_i, the mean centre is avg(array[x]), avg(array[y])\n",
    "\n",
    "In our case, we use **mean latitude** and **mean longitude** for each group, for example:\n",
    "- One mean centre per **month** and **crime category**.\n",
    "\n",
    "---\n",
    "\n",
    "##### What does it mean in spatial analysis?\n",
    "\n",
    "- It represents the **‚Äúcentre of gravity‚Äù** of a set of events.  \n",
    "- If the mean centre moves over time (e.g. month to month), this suggests a **shift in the typical location** of that crime type.  \n",
    "- Comparing mean centres between categories (e.g. burglary vs theft) shows whether different crimes tend to be concentrated in **similar or different parts of the city**.\n",
    "\n",
    "---\n",
    "\n",
    "##### What the results could mean\n",
    "\n",
    "- If the mean centre of burglary is consistently **north** of the mean centre of theft, this suggests that **burglary risk** is more concentrated in northern areas.  \n",
    "- If the mean centres for a category **wander a lot** over months, the crime type might be **spatially more mobile** or dispersed.  \n",
    "- If they hardly move, it suggests a more **stable core area** of risk.\n",
    "\n",
    "‚ö†Ô∏è **Limitations**:  \n",
    "The mean centre is very sensitive to **outliers** and does **not** tell you about the *spread* or *shape* of the distribution (only its central tendency).\n",
    "\n",
    "---\n",
    "\n",
    "##### üöÄ Task: Compute mean centres for each month and 3 highest crime categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267acc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Month and main_category,\n",
    "# compute average Latitude and Longitude for each combination\n",
    "\n",
    "df_mean_center = (\n",
    "    ___\n",
    "    .groupby([___])[[___]] # what columns do we group? What columns do we keep? \n",
    "    .___ # which aggregating function we want to do?\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Visualize the head of the dataframe\n",
    "df_mean_center.___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799ab0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise month-by-month mean centres of key categories in KeplerGl\n",
    "# You can filter the value in a dataframe column by using df[df['column'] == value]\n",
    "\n",
    "___ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b733d",
   "metadata": {},
   "source": [
    "> ‚ÅâÔ∏è Do the mean centres for theft, violence, and burglary overlap, or are they in different parts of the city?\n",
    ">\n",
    "> ‚ÅâÔ∏è Does any category seem to wander more over months (higher dispersion)? How could you quantify this dispersion? (Hint: distance from mean or variance of coordinates.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2352bd4",
   "metadata": {},
   "source": [
    "### E. Is the spatial distribution of crime random? (Global Moran‚Äôs I)\n",
    "\n",
    "##### üìö Concept: Global Moran‚Äôs I (overall spatial autocorrelation)\n",
    "\n",
    "**Global Moran‚Äôs I** is a measure of **spatial autocorrelation**, it tells us whether areas with similar values tend to be **near each other**.\n",
    "\n",
    "In our case, the value is the **crime count per hexagon**.\n",
    "\n",
    "- If high-count hexagons tend to be near other high-count hexagons,  \n",
    "  and low-count hexagons tend to be near low-count hexagons,  \n",
    "  we say the pattern is **positively autocorrelated** or **clustered**.\n",
    "- If high values tend to be near low values, the pattern is **negatively autocorrelated** or **checkerboard-like**.\n",
    "- If there is no clear pattern, the spatial distribution is close to **random**.\n",
    "\n",
    "Moran‚Äôs I is roughly interpreted as:\n",
    "\n",
    "- ( I > 0 ): similar values cluster together (positive autocorrelation).  \n",
    "- ( I approx 0 ): no spatial pattern (random-like).  \n",
    "- ( I < 0 ): neighbouring values tend to be dissimilar (negative autocorrelation).\n",
    "\n",
    "We also look at a **p-value** (usually from permutation tests) to decide if the observed I is **unlikely to occur by chance**.\n",
    "\n",
    "---\n",
    "\n",
    "##### What it means in the context of spatial crime analysis\n",
    "\n",
    "- A **positive and significant** Moran‚Äôs I (e.g. I = 0.3, p < 0.01) suggests that crime counts are **clustered**:  \n",
    "  high-crime cells are near other high-crime cells ‚Üí **hot areas** and **cold areas** exist.  \n",
    "- A value near zero with a high p-value suggests that the pattern is **not distinguishable from random**.  \n",
    "- A negative Moran‚Äôs I would indicate an alternating pattern (rare in crime data).\n",
    "\n",
    "---\n",
    "\n",
    "##### What the results could mean\n",
    "\n",
    "- A strong positive Moran‚Äôs I supports the common claim that **‚Äúcrime is not randomly distributed‚Äù** but concentrated in certain neighbourhoods or street segments.  \n",
    "- This justifies focusing more detailed analysis and interventions on **clusters of high crime**.  \n",
    "- If Moran‚Äôs I is weak or non-significant, hotspot analysis might be less meaningful, or the chosen **spatial scale** (hex size) may not be appropriate.\n",
    "\n",
    "Remember: Global Moran‚Äôs I is a **single number** summarising the **entire study area**; it does *not* tell you *where* the clusters are located.\n",
    "\n",
    "---\n",
    "\n",
    "##### üöÄ Tasks:\n",
    "- Use the hex grid GeoDataFrame (here called gdf_h3_crimes) with a point_count column.\n",
    "- Build K-nearest neighbours weights (each hex uses its 5 nearest neighbours).\n",
    "- Compute Moran‚Äôs I for the point_count field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32225ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy your h3 dataframe\n",
    "hex_for_moran = gdf_h3_crimes.copy()\n",
    "\n",
    "# change the type of your column crime count to float\n",
    "hex_for_moran['___'] = hex_for_moran['___'].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f9fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create spatial weights: 5 nearest neighbours for each hexagon\n",
    "# We are using pysal to create the weights, which is a library for spatial analysis\n",
    "# ps.weights.KNN uses the centroids of geometries by default.\n",
    "# You can also define your weight as Rook or Queen\n",
    "w = ps.weights.KNN.from_dataframe(___, k=___) # try different k value, what do you observe?\n",
    "# Replace KNN by Queen or Rook, what do you observe?\n",
    "\n",
    "# 2. Row-standardise the weights so each row sums to 1\n",
    "w.transform = \"R\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4376e0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compute global Moran's I on the crime count\n",
    "mi = esda.Moran(hex_for_moran[\"crimes\"], w) # use esda, which is a library for spatial analysis\n",
    "\n",
    "print(\"------ Global Moran's I on hexagon counts ------\")\n",
    "print(f\"Moran's I: {mi.I:.3f}\")\n",
    "print(f\"Expected I under randomness: {mi.EI:.3f}\")\n",
    "print(f\"p-value (permutation): {mi.p_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99432f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualize the data as a scatter plot \n",
    "# The library pysal allows to create a scatter plot of the data\n",
    "moran_scatterplot(mi, p=0.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de99d59",
   "metadata": {},
   "source": [
    "üí° **Interpretation**\n",
    "- If Moran‚Äôs I is clearly positive and the p-value is small (e.g. < 0.05),\n",
    "then crime counts are spatially clustered, not random.\n",
    "- If Moran‚Äôs I is near zero and p-value is large, the pattern is closer to random.\n",
    "\n",
    "> ‚ÅâÔ∏è Is your observed Moran‚Äôs I value closer to +1, 0, or ‚àí1? What does that mean substantively?\n",
    ">\n",
    "> ‚ÅâÔ∏è What does the p-value tell you about whether the clustering is statistically significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d346974",
   "metadata": {},
   "source": [
    "### F. Where exactly are the clusters? (Local Moran‚Äôs I)\n",
    "\n",
    "##### üìö Concept: Local Moran‚Äôs I (Anselin), where are the clusters?\n",
    "\n",
    "While **Global Moran‚Äôs I** tells us whether there is clustering *overall*,  \n",
    "**Local Moran‚Äôs I** (also called *Anselin Local Moran*) tells us **where** clusters and outliers are located.\n",
    "\n",
    "For each spatial unit (each hexagon in our case), Local Moran‚Äôs I compares:\n",
    "\n",
    "- The **value in the hex** (e.g. its crime count), and  \n",
    "- The **average value of its neighbours**.\n",
    "\n",
    "Based on this, each hexagon can be classified into:\n",
    "\n",
    "1. **High‚ÄìHigh (HH)**: high value, surrounded by high values ‚Üí **hot spot cluster**.  \n",
    "2. **Low‚ÄìLow (LL)**: low value, surrounded by low values ‚Üí **cold spot cluster**.  \n",
    "3. **High‚ÄìLow (HL)**: high value, surrounded by low values ‚Üí **spatial outlier** (a stand-alone hot cell).  \n",
    "4. **Low‚ÄìHigh (LH)**: low value, surrounded by high values ‚Üí **another type of outlier**.\n",
    "\n",
    "The method also gives a **p-value** for each hexagon, indicating whether its local pattern is statistically significant.\n",
    "\n",
    "---\n",
    "\n",
    "##### What it means in spatial crime analysis\n",
    "\n",
    "- **High‚ÄìHigh** (HH) hexagons highlight **concentrated hot spots** ‚Äì areas where both the cell and its neighbourhood have high crime.  \n",
    "- **Low‚ÄìLow** (LL) hexagons indicate **clustered low-crime areas** (cool spots).  \n",
    "- **High‚ÄìLow** and **Low‚ÄìHigh** show **spatial outliers** that might be interesting for diagnosis (e.g. a single problematic block in an otherwise quiet area).\n",
    "\n",
    "Local Moran‚Äôs I is therefore a **localised decomposition** of global Moran‚Äôs I:  \n",
    "it breaks down the overall pattern into **location-specific stories**.\n",
    "\n",
    "---\n",
    "\n",
    "##### What the results could mean\n",
    "\n",
    "- Regions with many significant **High‚ÄìHigh** cells could be priority zones for **targeted interventions** (e.g. focused patrols, environmental design changes).  \n",
    "- **Low‚ÄìLow** clusters might be interpreted as relatively **safe areas**, possibly offering lessons about what works (good lighting, mixed land use, etc.).  \n",
    "- **Outliers** (HL or LH) might signal **special cases**, such as:\n",
    "  - a new emerging hotspot,  \n",
    "  - measurement issues,  \n",
    "  - or a localised crime generator (e.g. one problematic venue).\n",
    "\n",
    "‚ö†Ô∏è **Caution**: When many local tests are performed, some ‚Äúsignificant‚Äù clusters may occur **by chance** (multiple testing problem). Interpretation should be careful and contextual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46143d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Local Moran's I\n",
    "moran_loc = esda.Moran_Local( \n",
    "    hex_for_moran[___],\n",
    "    ___, # same weight as before\n",
    "    geoda_quads=True  # automatically gives 1,2,3,4 for HH, LL, LH, HL\n",
    ")\n",
    "\n",
    "# what is present in moran_loc?\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d261e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach results to GeoDataFrame\n",
    "___[\"moran_cat\"] = moran_loc.q               # quadrant category\n",
    "___[\"moran_zscore\"] = moran_loc.z_sim        # z-score of Local Moran\n",
    "___[\"moran_pvalue\"] = moran_loc.p_sim        # p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b40510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the result with moran_scatterplot\n",
    "___(moran_loc, ___)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d85b730",
   "metadata": {},
   "source": [
    "> ‚ÅâÔ∏è What is the difference compared to your previous plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32121461",
   "metadata": {},
   "source": [
    "Classifying clusters in a simple categorical field\n",
    "\n",
    "We create a new column cluster_moran with labels:\n",
    "- \"High-High_90\" for significant HH clusters at the 10% level\n",
    "- \"Low-Low_90\" for LL clusters\n",
    "- \"High-High_10\" for HH clusters at the 10% level\n",
    "- \"Low-Low_10\" for LL clusters\n",
    "- \"Not_Significant\" for non-significant clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d852793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define default value for new column\n",
    "hex_for_moran[\"cluster_moran\"] = \"Not_Significant\"\n",
    "\n",
    "# Assign the other values by creating a mask and using .loc on a specific column\n",
    "# mask for p value < 0.05 and category \"High-High\"\n",
    "mask = ___\n",
    "hex_for_moran.loc[mask, \"cluster_moran\"] = \"High-High_90\"\n",
    "\n",
    "# do the same for the other values\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53549cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results using kepler \n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c71a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can create a quick visualization with .plot()\n",
    "ax = hex_for_moran.plot(\n",
    "    column=\"cluster_moran\", # column to plot\n",
    "    figsize=(8, 8),\n",
    "    legend=True,\n",
    "    categorical=True # colors are categorical\n",
    ")\n",
    "ax.set_title(\"Local Moran's I cluster types (High-High, Low-Low, etc.)\")\n",
    "ax.set_axis_off()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4bfea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also visualize the data directly in Plotly with an interactive map but it can be combersome\n",
    "\n",
    "# define the color mapping \n",
    "color_map = {\n",
    "    'Not_Significant': 'lightgray',\n",
    "    'High-High_90': 'lightcoral',\n",
    "    'Low-Low_90': 'lightblue',\n",
    "    'Low-High_90': 'blue',\n",
    "    'High-low_90': 'red',\n",
    "}\n",
    "\n",
    "# plot with attributes\n",
    "px.choropleth_map(\n",
    "    hex_for_moran, # dataframe to plot\n",
    "    geojson=hex_for_moran.geometry, # column geometry\n",
    "    locations=hex_for_moran.index, # index of each polygon\n",
    "    color='cluster_moran', # column to color\n",
    "    color_discrete_map=color_map, # color to use\n",
    "    hover_data=['moran_zscore', 'moran_pvalue', 'cluster_moran'], # data to show on hover\n",
    "    zoom=10, # zoom level\n",
    "    center={\"lat\": 41.8781, \"lon\": -87.6298}, # center of the map\n",
    "    height= 1000, # height of the map\n",
    "    map_style=\"light\", # style of the basemap\n",
    "    title=\"Anselin Local Moran'I of crimes count\" # title of the map\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021cc957",
   "metadata": {},
   "source": [
    "> ‚ÅâÔ∏è Where are the most prominent High-High areas located?\n",
    ">\n",
    "> ‚ÅâÔ∏è Can you identify any High-Low or Low-High outliers and think of possible explanations?\n",
    ">\n",
    "> ‚ÅâÔ∏è What are benefits/drawbacks of the different visualization methods we used? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b642be3",
   "metadata": {},
   "source": [
    "### G. Hot and cold spots with Getis‚ÄìOrd Gi*\n",
    "\n",
    "##### Concept: Getis‚ÄìOrd Gi*, direct hot spot and cold spot detection\n",
    "\n",
    "The **Getis‚ÄìOrd Gi\\*** statistic is another local measure used to identify **hot spots** and **cold spots**.\n",
    "\n",
    "Instead of comparing ‚Äúvalue vs. neighbours‚Äô average‚Äù (as Local Moran does), Gi\\* looks at:\n",
    "\n",
    "- The **sum of values** in a location and its neighbours, and  \n",
    "- Compares this sum to what would be expected **under spatial randomness**.\n",
    "\n",
    "The output is:\n",
    "\n",
    "- A **z-score** for each spatial unit (hexagon), and  \n",
    "- A **p-value** indicating whether this z-score is statistically significant.\n",
    "\n",
    "Interpretation of the z-score:\n",
    "\n",
    "- **High positive z-score** + low p-value ‚Üí significant **hot spot** (cluster of high values).  \n",
    "- **Large negative z-score** + low p-value ‚Üí significant **cold spot** (cluster of low values).  \n",
    "- z-scores near zero with high p-values ‚Üí not significantly different from random.\n",
    "\n",
    "---\n",
    "\n",
    "##### What it means in spatial crime analysis\n",
    "\n",
    "In a crime context, Gi\\* tells you:\n",
    "\n",
    "> ‚ÄúIs this hexagon part of a **local concentration of high crime counts** (hot spot)  \n",
    ">  or a **local concentration of low counts** (cold spot)?‚Äù\n",
    "\n",
    "It focuses strongly on areas where **high values reinforce each other** spatially.\n",
    "\n",
    "- Hot spots (high z, low p) are where high crime counts **pile up** spatially.  \n",
    "- Cold spots (low z, low p) are areas where low crime counts cluster (possibly safer zones).\n",
    "\n",
    "---\n",
    "\n",
    "##### What the results could mean\n",
    "\n",
    "- **Significant hot spots** highlight locations that may deserve **high priority** for prevention, enforcement, or situational interventions.  \n",
    "- **Significant cold spots** might be used as reference or **‚Äúcontrol‚Äù areas** ‚Äì places where crime is consistently low.  \n",
    "- Comparing Gi\\* results with Local Moran‚Äôs I:\n",
    "  - If both methods flag the same area as a hot spot, this **strengthens the evidence**.  \n",
    "  - If they disagree, it invites more reflection on **scale, neighbourhood definition, or data issues**.\n",
    "\n",
    "‚ö†Ô∏è **Caution**: Results depend on how you define ‚Äúneighbours‚Äù (distance band, k-nearest neighbours, etc.).  \n",
    "Different choices can slightly change which areas are labelled as hot or cold spots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97246710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualize local G (Gi*) on hexagon point counts\n",
    "# use the function G_Local from esda\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928326ab",
   "metadata": {},
   "source": [
    "> ‚ÅâÔ∏è Do the Gi* hot spots coincide with the Local Moran High-High clusters?\n",
    "\n",
    "> ‚ÅâÔ∏è Which method (Local Moran vs Gi*) do you find clearer to interpret for policing decisions, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a1e339",
   "metadata": {},
   "source": [
    "**Congratulations! üéâ You have successfully completed the exercise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a22c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line is to clear the output of the notebook, so that when you commit it, it is clean\n",
    "!jupyter nbconvert --clear-output --inplace crime_ex.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
