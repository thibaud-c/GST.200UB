{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8333a1",
   "metadata": {},
   "source": [
    "# Crime Hotspot Mapping and Prediction Accuracy Index (PAI)\n",
    "\n",
    "\n",
    "> Chainey, S., Tompson, L., & Uhlig, S. (2008b). The utility of hotspot mapping for predicting spatial patterns of crime. Security Journal, 21(1â€“2), 4â€“28. [10.1057/palgrave.sj.8350066](https://doi.org/10.1057/palgrave.sj.8350066)\n",
    "\n",
    "\n",
    "## ðŸŽ¯ Objectives\n",
    "\n",
    "- Explore a real crime data set with Python and GeoPandas.  \n",
    "- Create different types of **hotspot maps** (districts, beats, hexagonal grid, and clusters).  \n",
    "- Implement and interpret the **Prediction Accuracy Index (PAI)** as proposed in crime analysis research (Chainey, Tompson & Uhlig, 2008).  \n",
    "- Reflect on how different hotspot methods perform at predicting where crime will occur next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7531085",
   "metadata": {},
   "source": [
    "### Before you start\n",
    "\n",
    "Answer these short questions in your own words (just a sentence each):\n",
    "\n",
    "1. What do you think a **crime hotspot** is?  \n",
    "2. Why might police care about **predicting** where crime will happen, rather than only mapping where it already happened?  \n",
    "3. What is one potential **risk** of basing decisions on hotspot maps?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0113f3",
   "metadata": {},
   "source": [
    "## Part 0: Setup\n",
    "\n",
    "- (Optionally) install missing packages.  \n",
    "- Import the Python libraries used throughout the notebook.  \n",
    "- Define some **coordinate reference systems (CRS)** and file paths.\n",
    "\n",
    "### Data\n",
    "\n",
    "For reproducibility, use an extract of the **Chicago â€œCrimes â€“ 2001 to Presentâ€** dataset\n",
    "from the City of Chicago Open Data Portal.\n",
    "\n",
    "1. Open the City of Chicago Open Data Portal.  \n",
    "2. Search for **[Crimes â€“ 2001 to Present](https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-Present/ijzp-q8t2/about_data)**.  \n",
    "3. Filter (for example) to the year 2025 and export as CSV.  \n",
    "4. Save it locally as: `data/chicago_crimes_sample.csv`.\n",
    "5. Download also the districts and the beats of Chicago\n",
    "\n",
    "> You may choose a different filename or time period â€“ just remember to update the path\n",
    "> and filters in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d279b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: install missing libraries in your environment\n",
    "# Run this cell **only if** you get ImportError messages below.\n",
    "# Remove the leading `#` before %pip to actually install.\n",
    "\n",
    "# %pip install pandas geopandas shapely numpy matplotlib plotly keplergl h3 scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1074f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data handling\n",
    "import pandas as pd        \n",
    "import numpy as np         # numerical operations and arrays\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# Visualisation\n",
    "from keplergl import KeplerGl  \n",
    "\n",
    "# Spatial analysis\n",
    "import h3 # Hexagonal grid indexing\n",
    "from sklearn.cluster import DBSCAN # Clustering for hotspot detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694e7cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "\n",
    "# Global CRS settings\n",
    "GLOBAL_CRS = \"EPSG:4326\"   # WGS84 geographic coordinates (lat/lon)\n",
    "METRIC_CRS = \"EPSG:3857\"   # Web Mercator (units ~ metres) -> you can also use EPSG:2163 for a projected US Albers\n",
    "\n",
    "# File paths (adapt if your files are in a subfolder like './data/...')\n",
    "CRIMES_DATA_PATH = ___\n",
    "DISTRICT_DATA_PATH = ___\n",
    "BEATS_DATA_PATH = ___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dfcf5d",
   "metadata": {},
   "source": [
    "## Part 1: Density and PAI estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b35c76",
   "metadata": {},
   "source": [
    "### A. Load crime data and first exploration\n",
    "\n",
    "We start by:\n",
    "\n",
    "1. Loading the crime CSV file into a `pandas.DataFrame`.  \n",
    "2. Dropping clearly redundant coordinate columns (if present).  \n",
    "3. Removing records with missing coordinates.  \n",
    "4. Converting the table to a `GeoDataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f7bab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load crime data\n",
    "crimes = pd.read_csv(CRIMES_DATA_PATH)\n",
    "\n",
    "print(\"Original number of rows:\", ___)\n",
    "print(\"Columns:\", list(___))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc4775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant coordinate columns if present\n",
    "cols_to_drop = [___]\n",
    "df_crimes = ___.drop(columns=___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f55dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing lat/lon\n",
    "number_of_rows_before = ___\n",
    "df_crimes = df_crimes.dropna(subset=[___])\n",
    "number_of_rows_after\n",
    "print(\"Missing coordinates (%):\", ___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6584267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick descriptive statistics\n",
    "df_crimes.___()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf63df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the interactive visualization with Kepler\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c0a6ce",
   "metadata": {},
   "source": [
    "> â‰ï¸ How might **data quality issues** (missing coordinates, mis-typed districts, wrong timestamps) influence your hotspot analysis and any decisions based on it?\n",
    ">\n",
    "> â‰ï¸ After exploring the map: do you see any **clearly visible clusters** or corridors of crime?  \n",
    ">\n",
    "> â‰ï¸ How might the **underlying urban structure** (roads, land use, transport) explain what you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2de4c79",
   "metadata": {},
   "source": [
    "### B. Trainâ€“test split: preparing for prediction evaluation\n",
    "\n",
    "To evaluate hotspot maps as **predictors**, we follow the general idea from Chainey et al. (2008):\n",
    "\n",
    "- Use **historical crime data** (training period) to build hotspot maps.  \n",
    "- Use **future crime data** (test period) to evaluate how many test crimes fall inside the hotspots.\n",
    "\n",
    "Here we use a **simple split**:\n",
    "\n",
    "- Training set: all crimes **before** the last 7 days.  \n",
    "- Test set: crimes in the **last 7 days** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6defe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Date' is a proper datetime column\n",
    "df_crimes[\"Date\"] = pd.to_datetime(___)\n",
    "\n",
    "max_date = ___\n",
    "last_week_start_date = max_date - pd.Timedelta(days=7)\n",
    "\n",
    "# To split out the dataframe, we will use a mask. A mask is a boolean condition (test)\n",
    "\n",
    "mask_test = df_crimes[\"Date\"] > last_week_start_date # here we want to filter all the columns that have a date > to the last_week_start_date\n",
    "\n",
    "df_test = df_crimes.loc[mask_test].copy()\n",
    "df_train = df_crimes.loc[___].copy() # here we want the opposite of our test mask. In pandas you can use `~` to get the opposite of a boolean condition \n",
    "\n",
    "print(f\"Training set: {___} crimes\")\n",
    "print(f\"Test set: {___} crimes\")\n",
    "\n",
    "print(\"Training period:\", ___, \"to\", ___)\n",
    "print(\"Test period:\", ___, \"to\", ___)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296d1540",
   "metadata": {},
   "source": [
    "### C. Loading district and beat boundaries\n",
    "\n",
    "We now load the polygon boundaries for:\n",
    "\n",
    "- **Districts** (larger administrative areas)  \n",
    "- **Beats** (smaller operational policing units)\n",
    "\n",
    "Both are stored as CSVs with a WKT geometry column called `the_geom`.\n",
    "\n",
    "---\n",
    "\n",
    "##### ðŸ“š Concept: administrative units and the MAUP\n",
    "\n",
    "Administrative units (districts, beats) are **not neutral**:\n",
    "\n",
    "- Their size and shape are products of history, politics, and operational needs.  \n",
    "- Changing the boundaries can change the **appearance** of a hotspot map.  \n",
    "- This relates to the **modifiable areal unit problem (MAUP)** â€“ results can change when you change the zoning or aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2607ef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load districts\n",
    "df_districts = ___\n",
    "\n",
    "# Convert WKT to geometry\n",
    "df_districts[\"geometry\"] = df_districts[\"the_geom\"].apply(wkt.loads)\n",
    "gdf_districts = gpd.GeoDataFrame(___, geometry=___, crs=___)\n",
    "\n",
    "print(\"Number of Districts:\", ___)\n",
    "\n",
    "# Quick plot\n",
    "gdf_districts.___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af800f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load beats\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b80d39",
   "metadata": {},
   "source": [
    "### D. Thematic mapping of geographic boundary areas\n",
    "\n",
    "One common hotspot method is to **count crimes per administrative unit** and shade polygons by that count.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Aggregate crimes to **districts** and map them.  \n",
    "2. Aggregate crimes to **beats** and map them.\n",
    "\n",
    "This corresponds to â€œthematic mapping of geographic boundary areasâ€ in the paper.\n",
    "\n",
    "--- \n",
    "\n",
    "##### ðŸ“š Concept: choropleth maps\n",
    "\n",
    "A **choropleth map** colours polygons based on a value (here: crime count).  \n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Very simple and widely understood.  \n",
    "- Easy to compute and update regularly.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Sensitive to how boundaries are drawn (MAUP).  \n",
    "- Does not show within-unit variation.\n",
    "- Large polygons with small populations can look very \"hot\".  \n",
    "\n",
    "Keep these pros and cons in mind when interpreting your maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e5b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate training crimes by district\n",
    "crime_dist_counts = ( # we write everything in parentheses to make it more readable and write over several lines\n",
    "    ___ # training data\n",
    "    .groupby(___) # group by district\n",
    "    .size() # count crimes per district\n",
    "    .reset_index(name=\"crimes_count\") # reset index and name the count column\n",
    ")\n",
    "\n",
    "# What does the aggregated data look like?\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbb05a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join to district polygons \n",
    "gdf_districts_with_crimes = ___.merge(\n",
    "    ___,\n",
    "    how=\"left\",\n",
    "    left_on=\"___\",\n",
    "    right_on=\"___\"\n",
    ")\n",
    "\n",
    "# Replace missing counts (districts with no crimes) with 0\n",
    "gdf_districts_with_crimes[\"crimes_count\"] = gdf_districts_with_crimes[\"crimes_count\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c418b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's show the result\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefe833a",
   "metadata": {},
   "source": [
    "> â‰ï¸ Which districts appear as the most intense hotspots?  \n",
    ">\n",
    "> â‰ï¸ Do these districts cover **large areas** or relatively small ones?  \n",
    ">\n",
    "> â‰ï¸ How might this affect resource allocation decisions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36f7eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same operation with the beats (it is the one we will use to calculate the PAI)\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74eeb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ’¡ You can already add 2 layers in your kepler map: \n",
    "# 1. The beats layer with their crime counts\n",
    "# 2. The point test layer\n",
    "# Try to visualize both layers at the same time!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defacecf",
   "metadata": {},
   "source": [
    "> â‰ï¸ Do the same areas appear as hotspots at both scales?  \n",
    ">\n",
    "> â‰ï¸ Where do the two maps **disagree**?  \n",
    ">\n",
    "> â‰ï¸ Which map do you find more **actionable** for operational policing, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62183594",
   "metadata": {},
   "source": [
    "### E. Grid / hexagonal hotspot mapping with H3\n",
    "\n",
    "Another common approach is to impose a **regular grid** over the city and count crimes in each cell.\n",
    "\n",
    "We will use:\n",
    "\n",
    "- The **H3 hexagonal grid system** (https://h3geo.org)  \n",
    "- Each crime point will be assigned to a hexagon.  \n",
    "- We will then map the count of crimes per hexagon.\n",
    "\n",
    "---\n",
    "\n",
    "##### ðŸ“š Concept: what is H3?\n",
    "\n",
    "H3 is a **global hierarchical hexagonal grid system** originally developed at Uber. Key ideas:\n",
    "\n",
    "- The world is divided into **hexagonal cells** at different resolutions.  \n",
    "- Each cell has a unique **index** (e.g. `\"882a1072b9fffff\"`).  \n",
    "- Higher resolutions â†’ smaller hexagons (finer detail).  \n",
    "- Neighbouring cells have similar size and shape, which avoids some biases from irregular polygons.\n",
    "\n",
    "Why hexagons?\n",
    "\n",
    "- They have more **neighbours** than squares (6 vs 4), reducing directional bias.  \n",
    "- They cover space more smoothly than many other shapes.  \n",
    "- They are popular in spatial statistics and ecological modelling.\n",
    "\n",
    "In hotspot analysis, H3 offers:\n",
    "\n",
    "- A **neutral, regular** spatial unit (not tied to administrative boundaries).  \n",
    "- Easy multi-scale analysis by changing the resolution.\n",
    "\n",
    "We will map crimes to hexagons at a single resolution as a first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91abc6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "H3_RESOLUTION = 8  # smaller number = bigger hexagons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a32484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will define 2 functions: one for assigning a hexagon to each geometry, and one to extract the geometry of the hexagons\n",
    "def get_hex_id(geometry, resolution=H3_RESOLUTION):\n",
    "    \"\"\"Assign a hexagon ID to a geometry.\"\"\"\n",
    "    if geometry is None or geometry.is_empty:\n",
    "        return None\n",
    "    return h3.latlng_to_cell(geometry.___, geometry.___, resolution)\n",
    "\n",
    "\n",
    "def get_hex_geometry(hex_id):\n",
    "    \"\"\"Get the geometry of a hexagon from its ID.\"\"\"\n",
    "\n",
    "    boundary = h3.cell_to_boundary(hex_id) # unfortunately, h3 returns a list of reverted coordinates and not a geometry\n",
    "\n",
    "    # swap to (lon, lat) for shapely\n",
    "    coords = [(lon, lat) for lat, lon in boundary]\n",
    "    \n",
    "    return Polygon(coords)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f54d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of your dataframe to keep the original clean\n",
    "df_h3_train = gdf_train.copy()\n",
    "\n",
    "# transform our df in a GeoDataFrame \n",
    "gdf_h3_train = gpd.GeoDataFrame(___, geometry=gpd.points_from_xy(___), crs=___)\n",
    "\n",
    "# assign a hex_id in each row based on your geometry\n",
    "\n",
    "gdf_h3_train[\"h3_cell\"] = gdf_h3_train[___].apply(___)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df4f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group and Count crimes per cell\n",
    "h3_crimes = (\n",
    "    ___\n",
    ") # we first group the hex to optimize the calculation of the geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f631dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build hexagon geometries\n",
    "h3_crimes[\"geometry\"] = h3_crimes[___].apply(___)\n",
    "\n",
    "# recreate a geodataframe\n",
    "gdf_h3_crimes = ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb7684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's show the result (h3 grid and points from test layer)\n",
    "___\n",
    "\n",
    "# What geometry is taken by default by Kepler? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12d4b65",
   "metadata": {},
   "source": [
    "> â‰ï¸ Visually, do the last-week crimes tend to fall inside or outside the **densest hexagons**?  \n",
    ">\n",
    "> â‰ï¸ How does your visual impression compare with the PAI values you will compute later?\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbbd72b",
   "metadata": {},
   "source": [
    "### F. DBSCAN clustering\n",
    "\n",
    "As a simple unsupervised method, we can use **DBSCAN** to find clusters of crime points.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Take a random sample of training crimes (to keep computation light).  \n",
    "2. Project to a metric CRS (so distances are in metres).  \n",
    "3. Run DBSCAN with an `eps` value in metres.  \n",
    "4. Visualise the clusters.\n",
    "\n",
    "---\n",
    "\n",
    "##### ðŸ“š Concept: what is DBSCAN?\n",
    "\n",
    "DBSCAN (**D**ensity-**B**ased **S**patial **C**lustering of **A**pplications with **N**oise) is a clustering algorithm with two key parameters:\n",
    "\n",
    "- **`eps`** â€“ radius of the neighbourhood (here, in metres).  \n",
    "- **`min_samples`** â€“ minimum number of points required to form a dense region.\n",
    "\n",
    "The algorithm:\n",
    "\n",
    "1. For each point, count how many neighbours it has within distance `eps`.  \n",
    "2. If it has at least `min_samples` neighbours, it is a **core point**.  \n",
    "3. Clusters are formed by connecting core points and their nearby neighbours.  \n",
    "4. Points that are not part of any cluster are labelled as **noise** (`-1`).\n",
    "\n",
    "Why DBSCAN is useful for crime analysis:\n",
    "\n",
    "- It discovers clusters of **arbitrary shape** (not just circular).  \n",
    "- It can identify **noise** (isolated incidents).  \n",
    "- You do not need to choose the **number of clusters** in advance.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "- Results are sensitive to the choice of `eps` and `min_samples`.  \n",
    "- A single pair of parameters may not work well across all areas (dense city centre vs sparse suburbs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce96322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN can be very heavy on computation for large datasets\n",
    "# Let's reduce our dataset to make it manageable\n",
    "\n",
    "# Extract a subsample of our dataframe\n",
    "dbscan_crimes = gdf_test.sample(frac=0.1, random_state=42) # frac=0.1 takes 10% of the data, random_state ensures reproducibility\n",
    "\n",
    "# Project to a metric CRS for distance calculations\n",
    "___\n",
    "\n",
    "# DBSCAN takes a set/stack of coordinates (from numpy) and not a dataframe. \n",
    "# Extract coodinates of our sample\n",
    "coords = np.array( # create a numpy array\n",
    "            list( # convert to list\n",
    "                zip( # zip x and y coordinates\n",
    "                    ___, \n",
    "                    ___\n",
    "                )\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da837bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We imported sklearn that provide a plug and play solution to run a DBSCAN\n",
    "db = DBSCAN(\n",
    "    eps=500,          # neighbourhood radius in metres, try different values \n",
    "    min_samples=50,   # minimum points to form a cluster, try different values\n",
    "    n_jobs=-1,        # use all available cores\n",
    "    metric=\"euclidean\" # distance metric to use\n",
    ")\n",
    "\n",
    "# Fit the DBSCAN model\n",
    "labels = db.fit_predict(coords)\n",
    "\n",
    "# Add cluster labels to the sample dataframe\n",
    "___\n",
    "\n",
    "print(\"Unique cluster IDs (âˆ’1 = noise):\", ___.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c33aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's show the result (DBSCAN and points from test layer)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90e8cf",
   "metadata": {},
   "source": [
    "> â‰ï¸ What happens if you **increase** `eps` while keeping `min_samples` constant?  \n",
    ">\n",
    "> â‰ï¸ What happens if you **decrease** `min_samples` while keeping `eps` constant?  \n",
    ">\n",
    "> â‰ï¸ How do your identified clusters compare to the choropleth maps or the H3 hexagon areas you created earlier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bee5b0",
   "metadata": {},
   "source": [
    "### G. Prediction Accuracy Index (PAI)\n",
    "\n",
    "We now want to **evaluate** how good our hotspot maps are at predicting where crime will occur next.\n",
    "\n",
    "Following Chainey et al. (2008), we use the **Prediction Accuracy Index (PAI)**:\n",
    "\n",
    "\n",
    "PAI = ( n / N ) / ( a / A )\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "- n = number of **test crimes** that fall inside hotspot areas  \n",
    "- N = total number of test crimes in the study area  \n",
    "- a = area of hotspots (e.g. total area of selected hexagons / beats / clusters)  \n",
    "- A = total area of the study area\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- **PAI = 1** â†’ hotspot performs like random selection.  \n",
    "- **PAI > 1** â†’ hotspot is better than random (good).  \n",
    "- **Higher PAI** â†’ more crimes captured in a smaller area.  \n",
    "\n",
    "We will:\n",
    "\n",
    "1. Compute the **study area** (union of all districts).  \n",
    "2. Turn the test-period crimes into a GeoDataFrame in the metric CRS.  \n",
    "3. Define a helper function to compute PAI.  \n",
    "4. Apply it to:\n",
    "   - Top 15% beats by crime count  \n",
    "   - Top 15% hexagons by crime count  \n",
    "   - DBSCAN clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8210ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total study area (in metric CRS) -> we can use an aggregation of the districts\n",
    "# first to calculate a metric value we need to change the CRS of our dataframe\n",
    "gdf_districts_metric = ___\n",
    "study_area = gdf_districts_metric.geometry.___\n",
    "\n",
    "\n",
    "print(f\"Total study area: {study_area/1e6:.2f} kmÂ²\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e0eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our test df in gdf (and change the crs)\n",
    "___\n",
    "\n",
    "# count the total number of crimes in our test set\n",
    "total_test_crimes = ___\n",
    "\n",
    "print(f\"Total number of test crimes: {total_test_crimes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bb0f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a helper function to compute PAI\n",
    "def compute_pai(hits, hotspot_area, total_test_crimes, study_area):\n",
    "    \"\"\"Compute Prediction Accuracy Index (PAI) for a given set of hotspot polygons.\n",
    "\n",
    "    hits: number of test crimes within hotspots\n",
    "    hotspot_area: total area of hotspot polygons in mÂ²\n",
    "    total_test_crimes: total number of test crimes\n",
    "    study_area: total area of study region (in mÂ²)\n",
    "    \"\"\"\n",
    "\n",
    "    # Hit rate (n / N)\n",
    "    hit_rate = hits / total_test_crimes\n",
    "\n",
    "    # Area percentage (a / A)\n",
    "    area_pct = hotspot_area / study_area\n",
    "\n",
    "    pai = (hit_rate / area_pct) if area_pct > 0 else np.nan\n",
    "\n",
    "    return pai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cf2166",
   "metadata": {},
   "source": [
    "#### G.1 PAI for beat-based hotspots\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Normalise crime counts by the maximum.  \n",
    "2. Select the **top 15%** beats.  \n",
    "3. Compute PAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bb7121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy to avoid modifying earlier GeoDataFrame\n",
    "beats_hotspots = gdf_beats_with_crimes.copy()\n",
    "\n",
    "# Normalize crime counts by maximum\n",
    "beats_hotspots[\"norm_crime\"] = ___\n",
    "\n",
    "# Top 15% beats by crime intensity\n",
    "threshold_beat = beats_hotspots[\"norm_crime\"].quantile(0.85) # you can get the value at a specific quantile with .quantile()\n",
    "# Create a mask to select top beats\n",
    "mask_top_beat = ___\n",
    "top_beats = beats_hotspots.loc[mask_top_beat].copy()\n",
    "\n",
    "print(\"Number of beat hotspots:\", ___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d2ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute area of top beats\n",
    "# change crs to metric\n",
    "top_beats = ___\n",
    "top_beats_area = top_beats.geometry.___\n",
    "\n",
    "print(f\"Total area of top beats: {top_beats_area/1e6:.2f} kmÂ²\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74662342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the hits within the top beats area\n",
    "beat_hits = gdf_test_metric.within(top_beats.geometry.unary_union).sum() # .unary_union combines all geometries into one, within() checks if points are within the union\n",
    "\n",
    "print(f\"Hits within top beats: {___}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c3a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PAI\n",
    "pai_beats = compute_pai(____)\n",
    "\n",
    "print(f\"BEATS PAI: {___:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5354a6de",
   "metadata": {},
   "source": [
    "#### G.2 PAI for H3 hexagon hotspots\n",
    "\n",
    "Similar to the beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f71bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the top h3 hotspots by area\n",
    "# identify and extract the top h3\n",
    "\n",
    "top_h3 = ___\n",
    "\n",
    "print(\"Number of h3 hotspots:\", ___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ce1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the area of the top h3 hotspots\n",
    "\n",
    "top_h3_area = ___\n",
    "\n",
    "print(f\"Total area of top beats: {top_h3_area/1e6:.2f} kmÂ²\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5c1b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the hits within the top h3 area\n",
    "\n",
    "h3_hits = ___ \n",
    "\n",
    "print(f\"Hits within top h3: {___}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aa4e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PAI\n",
    "pai_h3 = compute_pai(____)\n",
    "\n",
    "print(f\"H3 PAI: {___:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c69ce78",
   "metadata": {},
   "source": [
    "#### G.3 PAI for DBSCAN clusters\n",
    "\n",
    "To evaluate DBSCAN clusters as hotspots, we:\n",
    "\n",
    "1. Construct one polygon per cluster (convex hull).  \n",
    "2. Treat these polygons as hotspot areas.  \n",
    "3. Compute PAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ff978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the cluster geometry \n",
    "\n",
    "clusters = []\n",
    "\n",
    "for cid, group in dbscan_crimes.groupby(\"cluster_id\"): # Loop over each cluster \n",
    "    # cid is the cluster ID\n",
    "    # group is the set of crimes belonging to that cluster\n",
    "    \n",
    "    if cid == -1:\n",
    "        continue  # skip noise\n",
    "\n",
    "    hull = group.unary_union.convex_hull  # create one polygon per cluster\n",
    "\n",
    "    clusters.append({\n",
    "        \"cluster_id\": ___,\n",
    "        \"n_points\": ___,\n",
    "        \"geometry\": ___\n",
    "    })\n",
    "\n",
    "clusters_gdf = gpd.GeoDataFrame(___, geometry=___, crs=___)\n",
    "\n",
    "top_clusters = ___\n",
    "print(\"Number of hot points:\", top_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c0d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the area of the top cluster hotspots\n",
    "\n",
    "top_cluster_area = ___\n",
    "print(f\"Hotspot area: {top_cluster_area/1e6:.2f} kmÂ²\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd30554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the hits within the top h3 area\n",
    "\n",
    "cluster_hits = ___ \n",
    "\n",
    "print(f\"Hits within top h3: {___}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2686afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PAI\n",
    "pai_cluster = compute_pai(____)\n",
    "\n",
    "print(f\"H3 PAI: {___:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0b050f",
   "metadata": {},
   "source": [
    "## PART 2: Basic exploratory data analysis (EDA)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714f7d51",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-gst.200b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
